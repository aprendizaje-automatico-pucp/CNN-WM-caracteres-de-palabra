{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read word tables and write word lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with table of English  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Language_ID</th>\n",
       "      <th>Parameter_ID</th>\n",
       "      <th>Form</th>\n",
       "      <th>Segments</th>\n",
       "      <th>BorrowedScore</th>\n",
       "      <th>Essential</th>\n",
       "      <th>Borrowability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>13-1-1-1</td>\n",
       "      <td>13</td>\n",
       "      <td>1-100</td>\n",
       "      <td>world</td>\n",
       "      <td>w ɜː l d</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.401887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13-1-21-1</td>\n",
       "      <td>13</td>\n",
       "      <td>1-210</td>\n",
       "      <td>land</td>\n",
       "      <td>l æ n d</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.268285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13-1-212-1</td>\n",
       "      <td>13</td>\n",
       "      <td>1-212</td>\n",
       "      <td>soil</td>\n",
       "      <td>s ɔɪ l</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.099935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13-1-213-1</td>\n",
       "      <td>13</td>\n",
       "      <td>1-213</td>\n",
       "      <td>dust</td>\n",
       "      <td>d ʌ s t</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.233766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13-1-214-1</td>\n",
       "      <td>13</td>\n",
       "      <td>1-214</td>\n",
       "      <td>mud</td>\n",
       "      <td>m ʌ d</td>\n",
       "      <td>0.5</td>\n",
       "      <td>False</td>\n",
       "      <td>0.153974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Language_ID Parameter_ID   Form  Segments  BorrowedScore  \\\n",
       "0    13-1-1-1           13        1-100  world  w ɜː l d            0.0   \n",
       "1   13-1-21-1           13        1-210   land   l æ n d            0.0   \n",
       "2  13-1-212-1           13        1-212   soil    s ɔɪ l            1.0   \n",
       "3  13-1-213-1           13        1-213   dust   d ʌ s t            0.0   \n",
       "4  13-1-214-1           13        1-214    mud     m ʌ d            0.5   \n",
       "\n",
       "   Essential  Borrowability  \n",
       "0      False       0.401887  \n",
       "1      False       0.268285  \n",
       "2       True       0.099935  \n",
       "3      False       0.233766  \n",
       "4      False       0.153974  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tables/English.tsv', sep='\\t')\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1475\n"
     ]
    }
   ],
   "source": [
    "words = df.Segments.tolist()\n",
    "print(len(words))\n",
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 'data/English.txt'\n",
    "with open(out_path, 'w') as out:\n",
    "    for word in words:\n",
    "        out.write(' '+word+' \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make generic for all tables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def makeWordData(language):\n",
    "    df = pd.read_csv('tables/'+language+'.tsv', sep='\\t')\n",
    "    words = df.Segments.tolist()\n",
    "    print('Language =', language,'; size =', len(df), '; len(words) =', len(words))\n",
    "\n",
    "    with open('data/'+language+'.txt', 'w') as out:\n",
    "        for word in words:\n",
    "            out.write(' '+word+' \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language = English ; size = 1475 ; len(words) = 1475\n"
     ]
    }
   ],
   "source": [
    "makeWordData('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language = Hup ; size = 1179 ; len(words) = 1179\n",
      "Language = Imbabura Quechua ; size = 1319 ; len(words) = 1319\n",
      "Language = Mapudungun ; size = 1412 ; len(words) = 1412\n",
      "Language = Wichí ; size = 1361 ; len(words) = 1361\n"
     ]
    }
   ],
   "source": [
    "makeWordData('Hup')\n",
    "makeWordData('Imbabura Quechua')\n",
    "makeWordData('Mapudungun')\n",
    "makeWordData('Wichí')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate train, validate, test - English "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1475"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tables/'+'English'+'.tsv', sep='\\t')\n",
    "words = df.Segments.tolist()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031 222 222\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(words, test_size=0.15)\n",
    "train, valid = train_test_split(train, test_size=len(test))\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/'+'English'+'.train.txt', 'w') as tr_out:\n",
    "    for word in train:\n",
    "        tr_out.write(' '+word+' \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make generic all tables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def makeWordDatasets(language, split=0.15):\n",
    "    df = pd.read_csv('tables/'+language+'.tsv', sep='\\t')\n",
    "    words = df.Segments.tolist()\n",
    "    with open('data/'+language+'.txt', 'w') as out:\n",
    "        for word in words:\n",
    "            out.write(' '+word+' \\n')\n",
    "        \n",
    "    trainval, test = train_test_split(words, test_size=split)\n",
    "    print(language+' size of trainval, test =', len(trainval), len(test))\n",
    "    with open('data/'+language+'.trainval.txt', 'w') as out:\n",
    "        for word in trainval:\n",
    "            out.write(' '+word+' \\n')\n",
    "    with open('data/'+language+'.test.txt', 'w') as out:\n",
    "        for word in test:\n",
    "            out.write(' '+word+' \\n')\n",
    "            \n",
    "    train, valid = train_test_split(trainval, test_size=len(test))\n",
    "    print(language+' size of train, valid =', len(train), len(valid))\n",
    "    with open('data/'+language+'.train.txt', 'w') as out:\n",
    "        for word in train:\n",
    "            out.write(' '+word+' \\n')\n",
    "    with open('data/'+language+'.valid.txt', 'w') as out:\n",
    "        for word in valid:\n",
    "            out.write(' '+word+' \\n')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English size of trainval, test = 1253 222\n",
      "English size of train, valid = 1031 222\n"
     ]
    }
   ],
   "source": [
    "makeWordDatasets('English', split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hup size of trainval, test = 1002 177\n",
      "Hup size of train, valid = 825 177\n",
      "Imbabura Quechua size of trainval, test = 1121 198\n",
      "Imbabura Quechua size of train, valid = 923 198\n",
      "Mapudungun size of trainval, test = 1200 212\n",
      "Mapudungun size of train, valid = 988 212\n",
      "Wichí size of trainval, test = 1156 205\n",
      "Wichí size of train, valid = 951 205\n"
     ]
    }
   ],
   "source": [
    "makeWordDatasets('Hup', split=0.15)\n",
    "makeWordDatasets('Imbabura Quechua', split=0.15)\n",
    "makeWordDatasets('Mapudungun', split=0.15)\n",
    "makeWordDatasets('Wichí', split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify generic tables  \n",
    "- Reads all words into list  \n",
    "- Splits words on whitespace, dropping all whitespace including initial and final.\n",
    "\n",
    "### Determine largest segmented word size\n",
    "- largest is 22  \n",
    "- could keep with 35 to handle most other languages - basic vocabulary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'iː']\n"
     ]
    }
   ],
   "source": [
    "# Test to read and segment words.\n",
    "language = 'English'\n",
    "with open('data/'+language+'.train.txt', 'r') as into:\n",
    "    words = into.read().splitlines()\n",
    "#print(words)\n",
    "print(words[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "maxsz = 0\n",
    "for word in words:\n",
    "    sz = len(word.split())\n",
    "    maxsz = max(sz, maxsz)\n",
    "    \n",
    "print(maxsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "import glob , os\n",
    "os.chdir(\"./data\")\n",
    "files = glob.glob(\"*.txt\")\n",
    "    \n",
    "maxsz = 0\n",
    "for file in files:\n",
    "    with open(file, 'r') as into:\n",
    "        words = into.read().splitlines()\n",
    "        for word in words:\n",
    "            maxsz = max(maxsz, len(word.split()))\n",
    "            \n",
    "print(maxsz)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
